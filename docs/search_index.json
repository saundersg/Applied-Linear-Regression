[
["index.html", "Introduction 1 Introduction 1.1 Regression Cheat Sheet", " Introduction 1 Introduction Simple linear regression is a fancy term for “putting a line on a scatterplot of data.” For example, the line shown on the scatterplot below is often called a “linear regression line.” There is a rich mathematical theory behind linear regression. And experience has shown that the more deeply you connect with this theory, the more powerfully you can perform predictive data modeling and analysis. The goal of this course is to introduce you to this rich theory from a high level perspective. The more math you have studied prior to the course, the more deeply you will be able to connect with this “rich theory” behind linear regression. Still, even if you have but little experience in mathematics, this course should help you connect with the theory deeply enough that you will be able to appropriately use linear regression in applied applications. 1.1 Regression Cheat Sheet Past students have found the following table of terms, pronunciation guides, and so on to be useful reminders about things they learn in the course. You could skip over this table right now. It is most powerfully used by revisiting it as you work through each lesson in the course. Your eventual goal is to become very familiar with every element demonstrated within this table. Term LaTeX Pronunciation Meaning Math Notation R Code \\(Y_i\\) $Y_i$ “why-eye” The data \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\sigma^2)\\) YourDataSet$YourYvariable \\(\\hat{Y}_i\\) $\\hat{Y}_i$ “why-hat-eye” The fitted line \\(\\hat{Y}_i = b_0 + b_1 X_i\\) lmObject$fitted.values \\(E\\{Y_i\\}\\) $E\\{Y_i\\}$ “expected value of why-eye” True mean y-value \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) &lt;none&gt; \\(\\beta_0\\) $\\beta_0$ “beta-zero” True y-intercept &lt;none&gt; &lt;none&gt; \\(\\beta_1\\) $\\beta_1$ “beta-one” True slope &lt;none&gt; &lt;none&gt; \\(b_0\\) $b_0$ “b-zero” Estimated y-intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 &lt;- coef(lmObject)[1] \\(b_1\\) $b_1$ “b-one” Estimated slope \\(b_1 = \\frac{\\sum X_i(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\) b_1 &lt;- coef(lmObject)[2] \\(\\epsilon_i\\) $\\epsilon_i$ “epsilon-eye” Distance of dot to true line \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) &lt;none&gt; \\(r_i\\) $r_i$ “r-eye” or “residual-eye” Distance of dot to estimated line \\(r_i = Y_i - \\hat{Y}_i\\) lmObject$residuals \\(\\sigma^2\\) $\\sigma^2$ “sigma-squared” Variance of the \\(\\epsilon_i\\) \\(Var\\{\\epsilon_i\\} = \\sigma^2\\) &lt;none&gt; \\(MSE\\) $MSE$ “mean squared error” Estimate of \\(\\sigma^2\\) \\(MSE = \\frac{SSE}{n-p}\\) sum( lmObject$res^2 ) / (n - p) \\(SSE\\) $SSE$ “sum of squared error” (residuals) Measure of dot’s total deviation from the line \\(SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) sum( lmObject$res^2 ) \\(SSR\\) $SSR$ “sum of squared regression error” Measure of line’s deviation from y-bar \\(SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\) sum( (lmObject$fit - mean(YourData$Y))^2 ) \\(SSTO\\) $SSTO$ “total sum of squares” Measure of total variation in Y \\(SSR + SSE = SSTO = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\) sum( (YourData$Y - mean(YourData$Y)^2 ) \\(R^2\\) $R^2$ “R-squared” Proportion of variation in Y explained by the regression \\(R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\) summary(lmObject)$sigma^2 \\(\\hat{Y}_h\\) $\\hat{Y}_h$ “why-hat-aitch” Estimated mean y-value for some x-value called \\(X_h\\) \\(\\hat{Y}_h = b_0 + b_1 X_h\\) predict(lmObject, data.frame(XvarName=#)) \\(X_h\\) $X_h$ “ex-aitch” Some x-value, not necessarily one of the \\(X_i\\) values used in the regression \\(X_h =\\) some number Xh = # Confidence Interval &lt;none&gt; “confidence interval” Estimated bounds at a certain level of confidence for a parameter \\(b_0 \\pm t^* \\cdot s_{b_0}\\) or \\(b_1 \\pm t^* \\cdot s_{b_1}\\) confint(mylm, level = someConfidenceLevel) Parameter Estimate \\(\\beta_0\\) \\(b_0\\) \\(\\beta_1\\) \\(b_1\\) \\(\\epsilon_i\\) \\(r_i\\) \\(\\sigma^2\\) \\(MSE\\) \\(\\sigma\\) \\(\\sqrt{MSE}\\), the Residual standard error "],
["the-mathematical-model.html", "2 The Mathematical Model 2.1 Equation 1: The True Line 2.2 Part 2: The Dots 2.3 Part 3: The Estimated Line", " 2 The Mathematical Model \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(E\\{Y_i\\}\\)… Linear regression uses a mathematical function (\\(\\beta_0 + \\beta_1 X_i\\)) and a random error term (\\(\\epsilon_i\\)) to describe the regression relation between a response variable \\(Y\\) and an explanatory variable \\(X\\). The Mathematical Model for Simple Linear Regression \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0,\\sigma^2) \\] In this mathematical model, notice the use of the index \\(i\\) next to \\(Y\\), \\(X\\), and \\(\\epsilon\\). This index value \\(i\\) is a way of stating that each individual “\\(i\\)” (like you or me) is assumed to have a unique \\(Y\\)-value (\\(Y_i\\)), \\(X\\)-value (\\(X_i\\)), and \\(\\epsilon\\)-value (\\(\\epsilon_i\\)). On the other hand, there is no subscript \\(i\\) for the \\(\\beta_0\\) and \\(\\beta_1\\). That is because the mathematical function is assumed to be the same for all individuals involved in the regression. Similarly, there is no \\(i\\) on the variance term \\(\\sigma^2\\). That is because the regression model assumes that all individuals in the regression have the same variability of their error terms \\(\\epsilon_i\\). But this is already starting to become confusing. Let’s slow down and take a deeper look at this mathematical regression model. To really understand the mathematics of the regression model it is useful to consider the following three equations. 2.1 Equation 1: The True Line The first part of the mathematical model is what we might call the true line. It is most often called the “expected value of \\(Y_i\\),” denoted by, \\(E\\{Y_i\\}\\). (The \\(E\\) in \\(E\\{Y_i\\}\\) stands for “expected value.”) Thus, \\(E\\{Y_i\\}\\) is the mathematical function that defines the regression relation between \\(Y\\) and \\(X\\). The most important thing to understand about \\(E\\{Y_i\\}\\) is that it is an assumption that is made about how the data was “created.” It is something that is typically unknown in real life. \\(\\underbrace{E\\{Y\\}}_{\\substack{\\text{true mean} \\\\ \\text{y-value}}} = \\underbrace{\\overbrace{\\beta_0}^\\text{y-intercept} + \\overbrace{\\beta_1}^\\text{slope} X}_\\text{equation of a line}\\) Notice that the equation for \\(E\\{Y_i\\}\\) does not include the error term \\(\\epsilon_i\\). Thus, \\(E\\{Y_i\\}\\) is simply a mathematical function, in this case a line. The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us. The regression relation \\(E\\{Y\\} = \\beta_0 + \\beta_1 X\\) creates the line of regression where \\(\\beta_0\\) is the \\(y\\)-intercept of the line and \\(\\beta_1\\) is the slope of the line. The regression relationship provides the average \\(Y\\)-value, denoted \\(E\\{Y_i\\}\\), for a given \\(X\\)-value, denoted by \\(X_i\\). Note: \\(E\\{Y\\}\\) is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value. 2.2 Part 2: The Dots The dots, i.e., the data as shown by dots in a scatterplot are assumed to be created by the regression relation plus an error term: \\(Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{E\\{Y_i\\}} + \\underbrace{\\epsilon_i}_\\text{error term} \\quad \\text{where} \\ \\epsilon_i\\sim N(0,\\sigma^2)\\) This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual \\(i\\), denoted by \\(Y_i\\), was “created” by adding an error term \\(\\epsilon_i\\) to each individual’s “expected” value \\(\\beta_0 + \\beta_1 X_i\\). Note the “order of creation” would require first knowing an indivual’s x-value, \\(X_i\\), then their expected value from the regression relation \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) and then adding their \\(\\epsilon_i\\) value to the result. The \\(\\epsilon_i\\) allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance \\(\\epsilon_i\\) from the line. Note: \\(Y_i\\) is pronounced “why-eye” because it is the y-value for individual \\(i\\). Sometimes also called “why-sub-eye” because \\(i\\) is in the subscript of \\(Y\\). 2.3 Part 3: The Estimated Line The estimated line is the least squares regression line that is obtained directly from a sample of data. \\(\\underbrace{\\hat{Y}_i}_{\\substack{\\text{estimated mean} \\\\ \\text{y-value}}} = \\underbrace{b_0 + b_1 X_i}_\\text{estimated regression equation}\\) The estimated line is shown by the solid line in the graph below. \\(\\hat{Y}\\) is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation \\(E\\{Y\\}\\). So \\(\\hat{Y}\\) is interpreted as the estimated average (or mean) \\(Y\\)-value for any given \\(X\\)-value. Thus, \\(b_0\\) is the estimated y-intercept and \\(b_1\\) is the estimated slope. The b’s are sample statistics, like \\(\\bar{x}\\) and the \\(\\beta\\)’s are population parameters like \\(\\mu\\). The \\(b\\)’s estimate the \\(\\beta\\)’s. Note: \\(\\hat{Y}_i\\) is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from \\(b_0 + b_1 X_i\\). It is always different from \\(Y_i\\) because dots are rarely if ever exactly on the estimated regression line. This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line). Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line. We could loosely call this the “order of creation” as shown by the following diagram. par(mfrow=c(1,3), mai=c(.2,.2,.4,.1)) plot(y ~ x, col=&quot;white&quot;, main=&quot;A Law is Given&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;) curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, main=&quot;Data is Created&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) curve(beta0 + beta1*x, add=TRUE, lty=2) plot(y ~ x, pch=16, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;The Law is Estimated&quot;) curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;) curve(beta0 + beta1*x, add=TRUE, lty=2) A Law is Given Data is Created The Law is Estimated \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\) \\(Y_i = E\\{Y_i\\} + \\epsilon_i\\) \\(\\hat{Y}_i = b_0 + b_1 X_i\\) The true line is the “law”. The \\(Y_i\\) are created by adding \\(\\epsilon_i\\) to \\(E\\{Y_i\\}\\) where \\(E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i\\). The law is estimated with \\(\\hat{Y}_i\\) which is given with lm(...). Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”. ## Simulating Data from a Regression Model ## This R-chunk is meant to be played in your R Console. ## It allows you to explore how the various elements ## of the regression model combine together to &quot;create&quot; ## data and then use the data to &quot;re-create&quot; the line. set.seed(101) #Allows us to always get the same &quot;random&quot; sample #Change to a new number to get a new sample n &lt;- 30 #set the sample size X_i &lt;- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45. beta0 &lt;- 3 #Our choice for the y-intercept. beta1 &lt;- 1.8 #Our choice for the slope. sigma &lt;- 2.5 #Our choice for the std. deviation of the error terms. epsilon_i &lt;- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma. Y_i &lt;- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model fabData &lt;- data.frame(y=Y_i, x=X_i) #Store the data as data View(fabData) #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it. fab.lm &lt;- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData. summary(fab.lm) #Summarize your model. plot(y ~ x, data=fabData) #Plot the data. abline(fab.lm) #Add the estimated regression line to your plot. # Now for something you can&#39;t do in real life... but since we created the data... abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). legend(&quot;topleft&quot;, legend=c(&quot;True Line&quot;, &quot;Estimated Line&quot;), lty=c(2,1), bty=&quot;n&quot;) #Add a legend to your plot specifying which line is which. "],
["interpreting-the-model-parameters.html", "3 Interpreting the Model Parameters", " 3 Interpreting the Model Parameters \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope), estimated by \\(b_0\\) and \\(b_1\\), interpreted as… The interpretation of \\(\\beta_0\\) is only meaningful if \\(X=0\\) is in the scope of the model. If \\(X=0\\) is in the scope of the model, then the intercept is interpreted as the average y-value, denoted \\(E\\{Y\\}\\), when \\(X=0\\). The interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the average y-value, denoted \\(E\\{Y\\}\\), per unit change in \\(X\\). It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”. To better see this, consider the three graphics shown below. par(mfrow=c(1,3)) hist(mtcars$mpg, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Number of Vehicles&quot;, xlab=&quot;Gas Mileage (mpg)&quot;, col=&quot;skyblue&quot;) boxplot(mpg ~ cyl, data=mtcars, border=&quot;skyblue&quot;, boxwex=0.5, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Number of Cylinders of Engine (cyl)&quot;) plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=&quot;skyblue&quot;, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Quarter Mile Time (qsec)&quot;) abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=&quot;darkgray&quot;) mtext(side=3, text=&quot;Automatic Transmissions Only (am==0)&quot;, cex=0.5) abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=&quot;gray&quot;) The Histogram The Boxplot The Scatterplot The histogram on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09. The boxplot in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by \\(26.66 - 19.74 = 6.92\\) mpg, then by \\(19.74 - 15.1 = 4.64\\) mpg. The scatterplot on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value. "],
["residuals-and-errors.html", "4 Residuals and Errors", " 4 Residuals and Errors \\(r_i\\), the residual, estimates \\(\\epsilon_i\\), the true error… Residuals are the difference between the observed value of \\(Y_i\\) (the point) and the predicted, or estimated value, for that point called \\(\\hat{Y_i}\\). The errors are the true distances between the observed \\(Y_i\\) and the actual regression relation for that point, \\(E\\{Y_i\\}\\). We will denote a residual for individual \\(i\\) by \\(r_i\\), \\[ r_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{\\hat{Y}_i}_{\\substack{\\text{Predicted} \\\\ \\text{Y-value}}} \\quad \\text{(residual)} \\] The residual \\(r_i\\) estimates the true error for individual \\(i\\), \\(\\epsilon_i\\), \\[ \\epsilon_i = \\underbrace{Y_i}_{\\substack{\\text{Observed} \\\\ \\text{Y-value}}} - \\underbrace{E\\{Y_i\\}}_{\\substack{\\text{True Mean} \\\\ \\text{Y-value}}} \\quad \\text{(error)} \\] In summary… Residual \\(r_i\\) Error \\(\\epsilon_i\\) Distance between the dot \\(Y_i\\) and the estimated line \\(\\hat{Y}_i\\) Distance between the dot \\(Y_i\\) and the true line \\(E\\{Y_i\\}\\). \\(r_i = Y_i - \\hat{Y}_i\\) \\(\\epsilon_i = Y_i - E\\{Y_i\\}\\) Known Typically Unknown As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms. Keep in mind the idea that the errors \\(\\epsilon_i\\) “created” the data and that the residuals \\(r_i\\) are computed after using the data to “re-create” the line. Residuals have many uses in regression analysis. They allow us to diagnose the regression assumptions, See the “Assumptions” section below for more details. estimate the regression relation, See the “Estimating the Model Parameters” section below for more details. estimate the variance of the error terms, See the “Estimating the Model Variance” section below for more details. and assess the fit of the regression relation. See the “Assessing the Fit of a Regression” section below for more details. "],
["assessing-the-fit-of-a-regression.html", "5 Assessing the Fit of a Regression", " 5 Assessing the Fit of a Regression \\(R^2\\), SSTO, SSR, and SSE… Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line. A common way to measure the fit of a regression is with correlation. While this can be a useful measurement, there is greater insight in using the square of the correlation, called \\(R^2\\). Before you can understand \\(R^2\\), you must understand three important “sums of squares”. (Read more about sums…) A sum is just a fancy word for adding things together. \\[ 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Long sums get tedious to write out by hand. So we use the symbol \\(\\Sigma\\) to denote the word “sum”. Further, we use a subscript \\(\\Sigma_{i=1}\\) to state what value the sum is beginning with, and a superscript \\(\\Sigma_{i=1}^6\\) to state the value we are ending at. This gives \\[ \\sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21 \\] Test your knowledge, do you see why the answer is 6 to the sum below? \\[ \\sum_{i=1}^3 i = 6 \\] Computing sums in R is fairly easy. Type the following codes in your R Console. sum(1:6) #gives the answer of 21 sum(1:3) #gives the answer of 6 However, sums really become useful when used with a data set. Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by \\(i=3\\), has a speed of 7 and a dist of 4. pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3) Individual speed dist 1 4 2 2 4 10 3 7 4 4 7 22 5 8 16 6 9 10 To compute the sum of the speed column, use sum(speed). If we divided this sum by 6, we would get the mean of speed mean(speed). In fact, the two most used statistics mean(...) and sd(...) both use sums. Take a moment to review the formulas for mean and standard deviation. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly. … Welcome back. Suppose we let X = speed and Y = dist. Then \\(X_3 = 7\\) and \\(Y_3 = 4\\) because we are accessing row 3 of both the \\(X\\) (or speed) column and \\(Y\\) (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, sum(speed) would be written mathematically as \\(\\sum_{i=1}^6 X_i\\) and sum(dist) would be written as \\(\\sum_{i=1}^6 Y_i\\). Sum of Squared Errors Sum of Squares Regression Total Sum of Squares \\(\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\) \\(\\text{SSR} = \\sum_{i=1}^n \\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\) \\(\\text{SSTO} = \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2\\) Measures how much the residuals deviate from the line. Measures how much the regression line deviates from the average y-value. Measures how much the y-values deviate from the average y-value. Equals SSTO - SSR Equals SSTO - SSE Equals SSE + SSR sum( (Y - mylm$fit)^2 ) sum( (mylm$fit - mean(Y))^2 ) sum( (Y - mean(Y))^2 ) It is important to remember that SSE and SSR split up SSTO, so that \\[ \\text{SSTO} = \\text{SSE} + \\text{SSR} \\] This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works. The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called \\(R^2\\) (“r-squared”). R-Squared (\\(R^2\\)) \\[ \\underbrace{R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}}_\\text{Interpretation: Proportion of variation in Y explained by the regression.} \\] The smallest \\(R^2\\) can be is zero, and the largest it can be is 1. This is because \\(SSR\\) must be between 0 and SSTO, inclusive. "],
["residual-plots-and-regression-assumptions.html", "6 Residual Plots and Regression Assumptions 6.1 Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 6.2 Q-Q Plot of the Residuals: Checks Assumption #2 6.3 Residuals versus Order Plot: Checks Assumption #5 6.4 Problems from Failed Assumptions", " 6 Residual Plots and Regression Assumptions Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots… There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate. Each assumption is labeled in the regression equation below. The regression relation between \\(Y\\) and \\(X\\) is linear. The error terms are normally distributed with \\(E\\{\\epsilon_i\\}=0\\). The variance of the error terms is constant over all \\(X\\) values. The \\(X\\) values can be considered fixed and measured without error. The error terms are independent. Regression Equation \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1 \\overbrace{X_i}^\\text{#4}}_{\\text{#1}} + \\epsilon_i \\quad \\text{where} \\ \\overbrace{\\epsilon_i \\sim}^\\text{#5} \\overbrace{N(0}^\\text{#2}, \\overbrace{\\sigma^2}^\\text{#3}) \\] Residuals are used to diagnose departures from the regression assumptions. As shown above, the regression equation makes several claims, or assumptions, about the error terms \\(\\epsilon_i\\), specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) as shown here \\[ \\epsilon_i \\underbrace{\\sim}_{\\substack{\\text{Independent} \\\\ \\text{Errors}}} \\overbrace{N}^{\\substack{\\text{Normally} \\\\ \\text{distributed}}}(\\underbrace{0}_{\\substack{\\text{mean of} \\\\ \\text{zero}}}, \\underbrace{\\sigma^2}_{\\substack{\\text{Constant} \\\\ \\text{Variance}}}) \\] While the actual error terms (\\(\\epsilon_i\\)) are unknown in real life, the residuals (\\(r_i\\)) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not. 6.1 Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3 The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the \\(\\hat{Y}_i\\). The residuals are the \\(r_i\\). This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable. The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption. The linear relation is assumed to be satisfied if there are no apparent trends in the plot. The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values. The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance. 6.2 Q-Q Plot of the Residuals: Checks Assumption #2 The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption. There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot. Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution. 6.3 Residuals versus Order Plot: Checks Assumption #5 When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated. Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated. 6.4 Problems from Failed Assumptions There are various problems that can arise when certain of the regression assumptions are not satisfied. 6.4.1 Lack of Linearity When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful. The y-intercept estimate can be drastically off from its actual true value. Important model information is lost by trying to use a simple slope term \\(\\beta_1\\) to describe the model with respect to \\(X\\). The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be. P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line. *Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model. The plot below demonstrate these difficulties. 6.4.2 Non-normal Error Terms When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation \\(\\sigma\\). This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions. 6.4.3 Constant Variance Assumption Violated True Estimated Y-Intercept 14.2 13.82 Slope 3.5 3.768 Sigma 2.5 13.02 6.4.4 Independence Assumption Violated True Estimated Y-Intercept 14.2 15.51 Slope 3.5 3.427 Sigma 2.5 5.043 6.4.5 Outliers Present True Estimated Y-Intercept 14.2 12.44 Slope 3.5 3.505 Sigma 2.5 6.224 6.4.6 All Assumptions Satisfied # Create Data from a True Model n &lt;- 30 #sample size beta_0 &lt;- 14.2 #True y-intercept beta_1 &lt;- 3.5 #True slope X_i &lt;- runif(n, 0, 20) #Sample of X-values sigma &lt;- 2.5 #True standard deviation epsilon_i &lt;- rnorm(n, 0, sigma) #normally distributed errors Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i #Sample of Y-values from model # Plot the Data and Fitted Model mylm &lt;- lm(Y_i ~ X_i) #Fit Model to Data layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), widths=c(2,2,2), heights=c(4,2,2)) #create plot panel plot(Y_i ~ X_i, #Plot the data pch=16, col=&quot;darkgray&quot;, xlim=c(0,20), ylim=c(0,100), main=&quot;All Assumptions Satisfied&quot;) abline(mylm, col=&quot;gray&quot;) #Add fitted line to plot abline(beta_0, beta_1, #Add True line to plot col=&quot;gray&quot;, lty=2) #Summarize the Model Fit pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]), Slope = c(True = beta_1, Estimated = mylm$coef[[2]]), Sigma = c(True = sigma, Estimated = summary(mylm)$sigma))) True Estimated Y-Intercept 14.2 14.1 Slope 3.5 3.488 Sigma 2.5 2.897 #Add summary to plot legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot; (&quot;, beta_0, &quot;)&quot;), paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot; (&quot;, beta_1, &quot;)&quot;), paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot; (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;) #Draw diagnostic plots plot(mylm, which=1:2) plot(mylm$residuals, ylab=&quot;Residuals&quot;) mtext(&quot;Residuals vs Order&quot;, side=3) "],
["estimating-the-model-parameters.html", "7 Estimating the Model Parameters 7.1 Least Squares 7.2 Maximum Likelihood 7.3 Estimating the Model Variance", " 7 Estimating the Model Parameters How to get \\(b_0\\) and \\(b_1\\): least squares &amp; maximum likelihood… There are two approaches to estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) obtained from either method are identical. The estimates for the true parameter values \\(\\beta_0\\) and \\(\\beta_1\\) are typically denoted by \\(b_0\\) and \\(b_1\\), respectively, and are given by the following formulas. Parameter Estimate Mathematical Formula R Code Slope \\(b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2}\\) b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) Intercept \\(b_0 = \\bar{Y} - b_1\\bar{X}\\) b_0 &lt;- mean(Y) - b_1*mean(X) It is important to note that these estimates are entirely determined from the observed data \\(X\\) and \\(Y\\). When the regression equation is written using the estimates instead of the parameters, we use the notation \\(\\hat{Y}\\), which is the estimator of \\(E\\{Y\\}\\). Thus, we write \\[\\begin{equation} \\hat{Y}_i = b_0 + b_1 X_i \\end{equation}\\] which is directly comparable to the true, but unknown values \\[\\begin{equation} E\\{Y_i\\} = \\beta_0 + \\beta_1 X_i. \\label{exp} \\end{equation}\\] 7.1 Least Squares To estimate the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) using least squares, we start by defining the function \\(Q\\) as the sum of the squared errors, \\(\\epsilon_i\\). \\[ Q = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2 \\] Then we use the function Q as if it were a function of \\(\\beta_0\\) and \\(\\beta_1\\). Ironically, the values of \\(Y\\) and \\(X\\) are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of \\(\\beta_0\\) and \\(\\beta_1\\). This least squares applet is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of \\(Q\\) with respect to both \\(\\beta_0\\) and \\(\\beta_1\\). \\[ \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum (Y_i - \\beta_0 - \\beta_1X_i) \\] \\[ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum X_i(Y_i-\\beta_0-\\beta_1X_i) \\] Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize \\(Q\\) for a given set of data. After all the calculations are completed we find the values of the parameter estimators \\(b_0\\) and \\(b_1\\) (of \\(\\beta_0\\) and \\(\\beta_1\\), respectively) are as stated previously. 7.2 Maximum Likelihood The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of \\(\\beta_0\\) and \\(\\beta_1\\) which minime the least squares \\(Q\\) function, we choose the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the \\(Y_i\\) for all observations \\(i=1,\\ldots,n\\). We can do this rather simply by using the assumption that the errors, \\(\\epsilon_i\\) are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if \\(f(Y_i)\\) denotes the probability density function for \\(Y_i\\), then the joint probability density for all \\(Y_i\\), \\(f(Y_1,\\ldots,Y_n)\\) is given by \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) \\] Since each \\(Y_i\\) is assumed to be normally distributed with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\) (see model ()) we have that \\[ f(Y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{Y_i-\\beta_0-\\beta_1X_i}{\\sigma}\\right)^2\\right]} \\] which provides the joint probability as \\[ f(Y_1,\\ldots,Y_n) = \\prod_{i=1}^n f(Y_i) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] The likelihood function \\(L\\) is then given by consider the \\(Y_i\\) and \\(X_i\\) fixed and the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as the variables in the function. \\[ L(\\beta_0,\\beta_1,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right]} \\] Instead of taking partial derivatives of \\(L\\) directly (with respect to all parameters) we take the partial derivatives of the \\(\\log\\) of \\(L\\), which is easier to work with. In a similar, but more difficult calculation, to that of minimizing \\(Q\\), we obtain the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize the log of \\(L\\), and which therefore maximize \\(L\\). (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate \\(\\hat{\\sigma}^2\\) of \\(\\sigma^2\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n} \\] 7.3 Estimating the Model Variance Estimating \\(\\sigma^2\\) with MSE… As shown previously in the “Estimating Model Parameters” section of this page, we can obtain estimates for the model parameters \\(\\beta_0\\) and \\(\\beta_1\\) by using either least squares estimation or maximum likelihood estimation. Those estimates were given by the formulas \\[ b_1 = \\frac{\\sum X_i(Y_i-\\bar{Y})}{\\sum(X_i-\\bar{X})^2} \\quad \\text{(Unbiased Estimate of $\\beta_1$)} \\] \\[ b_0 = \\bar{Y} - b_1\\bar{X} \\quad \\text{(Unbiased Estimate of $\\beta_0$)} \\] It turns out that these estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. Unfortunately, this is not the case for the maximum likelihood estimate \\(\\widehat{\\sigma}^2\\) of the model variance \\(\\sigma^2\\). This estimate turns out to be a biased estimator. This means that it is consistently wrong in its estimates of \\(\\sigma^2\\). If we left the estimator alone, our estimates for \\(\\sigma^2\\) would always be wrong. This is bad. Fortunately, there is a way to fix it, and this corrected version of the estimator is what we will actually use in practice to estimate \\(\\sigma^2\\). Without going into all the details, to “fix” the biased estimator of \\(\\sigma^2\\) that is given to us through maximum likelihood estimation, we need to correct its denominator so that it properly represent the degrees of freedom associated with the numerator, which it does not currently. To find the correct degrees of freedom, we have to notice that the \\(\\hat{Y}_i\\) in the numerator of \\(\\widehat{\\sigma}^2\\) is defined by \\[\\begin{equation} \\widehat{Y}_i = b_0 + b_1X_i \\label{hatY} \\end{equation}\\] From this equation, we notice that two means, \\(\\bar{X}\\) and \\(\\bar{Y}\\), were estimated from the data in order to obtain \\(\\hat{Y}_i\\). (See the formulas for \\(b_0\\) and \\(b_1\\) above, and note how they use both \\(\\bar{X}\\) and \\(\\bar{Y}\\) in their calculation.) Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for \\(\\hat{\\sigma}^2\\) should be \\(n-2\\) instead of \\(n\\). Some incredibly long calculations will show that the “fixed” estimator \\[\\begin{equation} s^2 = MSE = \\frac{\\sum(Y_i-\\hat{Y}_i)^2}{n-2} \\quad \\text{(Unbiased Estimator of $\\sigma^2$)} \\end{equation}\\] is an unbiased estimator of \\(\\sigma^2\\). Here \\(MSE\\) stands for mean squared error, which is the most obvious name for a formula that squares the errors \\(Y_i-\\hat{Y}_i\\) then adds them up and divides by their degrees of freedom. Similarly, we call the numerator \\(\\sum(Y_i-\\hat{Y}_i)^2\\) the sum of the squared errors, denoted by \\(SSE\\). It is also important to note that the errors are often denoted by \\(r_i = Y_i-\\hat{Y}_i\\), the residuals. Putting this all together we get the following equivalent statements for \\(MSE\\). \\[\\begin{equation} s^2 = MSE = \\frac{SSE}{n-2} = \\frac{\\sum(Y_i-\\widehat{Y}_i)^2}{n-2} = \\frac{\\sum r_i^2}{n-2} \\end{equation}\\] As a final note, even though the expected value \\(E\\{MSE\\} = \\sigma^2\\), which shows \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\), it unfortunately isn’t true that \\(\\sqrt{MSE}\\) is an unbiased estimator of \\(\\sigma\\). This presents a few problems later on, but these are minimal enough that we can overlook the issue and move forward. The \\(\\sqrt{MSE}\\) is called the residual standard error. "],
["transformations.html", "8 Transformations 8.1 Y-Transformations 8.2 X-Transformations", " 8 Transformations \\(Y&#39;\\), \\(X&#39;\\), and returning to the original space… Y transformations are denoted by y-prime, written \\(Y&#39;\\), and consist of raising \\(Y\\) to some power called \\(\\lambda\\). \\[ Y&#39; = Y^\\lambda \\quad \\text{(Y Transformation)} \\] Value of \\(\\lambda\\) Transformation to Use R Code -2 \\(Y&#39; = Y^{-2} = 1/Y^2\\) lm(Y^-2 ~ X) -1 \\(Y&#39; = Y^{-1} = 1/Y\\) lm(Y^-1 ~ X) 0 \\(Y&#39; = \\log(Y)\\) lm(log(Y) ~ X) 0.5 \\(Y&#39; = \\sqrt(Y)\\) lm(sqrt(Y) ~ X) 1 \\(Y&#39; = Y\\) lm(Y ~ X) 2 \\(Y&#39; = Y^2\\) lm(Y^2 ~ X) Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of \\(\\lambda\\) to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise. 8.1 Y-Transformations 8.1.1 Scatterplot Recognition The following panel of scatterplots can give you a good feel for when to try different values of \\(\\lambda\\). 8.1.2 Box-Cox Suggestion The boxCox(...) function in library(car) can also be helpful on finding values of \\(\\lambda\\) to try. 8.1.3 An Example Suppose we were running a simple linear regression on the cars dataset. This would be done with the code cars.lm &lt;- lm(dist ~ speed, data=cars) summary(cars.lm) Notice the line doesn’t quite fit the data as well as we would hope. Instead, the data looks a little curved. cars.lm &lt;-lm(dist ~ speed,data=cars) plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1, xlab=&quot;Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;, main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;) mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5) legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;) abline(cars.lm, col=&quot;gray&quot;) Using the boxCox(...) function from library(car) we would compute the following to determine which Y-transformation would be most meaningful. library(car) boxCox(cars.lm) The output from the boxCox(...) function looks as follows. cars.lm &lt;-lm(dist ~ speed,data=cars) boxCox(cars.lm) This plot tells use to use the \\(\\lambda = 0.5\\) transformation, so that \\(Y&#39; = Y^0.5 = \\sqrt{Y}\\). (To see this yourself, see above in the “Box-Cox Suggestion” section, as well as on the “Scatterplot Recognition” section.) Now, a transformation regression is performed using sqrt(Y) in place of Y as follows: cars.lm.t &lt;- lm(sqrt(dist) ~ speed, data=cars) summary(cars.lm.t) Estimate Std. Error t value Pr(&gt; (Intercept) 1.277 0.4844 2.636 0.01126 speed 0.3224 0.02978 10.83 1.773e-14 Then, \\[ \\widehat{Y}_i&#39; = 1.277 + 0.3224 X_i \\] And replacing \\(\\hat{Y}_i&#39; = \\sqrt{\\hat{Y}_i}\\) we have \\[ \\sqrt{\\widehat{Y}_i} = 1.277 + 0.3224 X_i \\] Solving for \\(\\hat{Y}_i\\) gives \\[ \\widehat{Y}_i = (1.277 + 0.3224 X_i)^2 \\] Which, using curve((1.277 + 0.3224*x)^2, add=TRUE) (see code for details) looks like this: plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1, xlab=&quot;Speed of the Vehicle (mph) \\n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;, main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;) mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5) legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;) curve( (1.277 + 0.3224*x)^2, add=TRUE, col=&quot;firebrick&quot;) 8.2 X-Transformations X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them. The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated. The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation. "],
["inference-for-the-model-parameters.html", "9 Inference for the Model Parameters 9.1 t Tests 9.2 Confidence Intervals 9.3 F tests", " 9 Inference for the Model Parameters t test formulas, sampling distributions, confidence intervals, and F tests… We are sometimes interested in making inference about \\(\\beta_0\\), the y-intercept. However, most inference in regression is focused on the slope, \\(\\beta_1\\). Recall that the interpretation of \\(\\beta_1\\) is the amount of increase (or decrease) in the expected value (average value) of \\(Y\\) per unit change in \\(X\\). Two types of inference about \\(\\beta_1\\), or similarly \\(\\beta_0\\) when applicable, are of interest. Hypotheses Test Statistic P-value \\(H_0: \\beta_0 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_0\\) \\(\\,\\neq\\,\\) You could use \\(&gt;\\) or \\(&lt;\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_0 - \\overbrace{0}^\\text{a number}}{s_{b_0}}\\] This is the formula for the test statistic. It measures how far the estimated y-intercept \\(b_0\\) is from the null hypothesis for \\(\\beta_0\\) in units of “standard errors of \\(b_0\\)”. Thus the division by \\(s_{b_0}\\). Though the hypothesized value of \\(\\beta_0\\) is typically 0, it could be any number. \\(H_0: \\beta_1 =\\) \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\(H_a: \\beta_1\\) \\(\\,\\neq\\,\\) You could use \\(&gt;\\) or \\(&lt;\\) instead of \\(\\neq\\) for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses \\(\\neq\\). \\(\\underbrace{0}_\\text{a number}\\) This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown. \\[t = \\frac{b_1 - \\overbrace{0}^\\text{a number}}{s_{b_1}}\\] This is the formula for the test statistic. It measures how far the estimated slope \\(b_1\\) is from the null hypothesis for \\(\\beta_1\\) in units of “standard errors of \\(b_1\\)”. Thus the division by \\(s_{b_1}\\). Though the hypothesized value of \\(\\beta_1\\) is typically 0, it could be any number. Left-tailed p-value = pt(-abs(tvalue), degrees of freedom). Double it to get the two-sided p-value. Confidence Interval Formula Standard Error \\(\\beta_0\\) \\(b_0 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_0\\). \\(s_{b_0}\\) The standard error of \\(b_0\\), denoted by \\(s_{b_0}\\) is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below. \\[s^2_{b_0} = MSE\\left[\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\] This is called the “estimated variance of \\(b_0\\)”. Taking the square root of this number gives the “standard error of \\(b_0\\)”. \\(\\beta_1\\) \\(b_1 \\pm\\) \\(t^*\\) This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom \\(n-p\\) (sample size - number of parameters in the regression model). \\(\\cdot\\) The critical value is multiplied by the standard error of \\(b_1\\). \\(s_{b_1}\\) The standard error of \\(b_1\\), denoted by \\(s_{b_1}\\) is provided in the regression summary output under the column header called “Std. Error”. It is calculated using the formula shown below. \\[s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\] This is called the “estimated variance of \\(b_1\\)”. Taking the square root of this number gives the “standard error of \\(b_1\\)”. To be more exact, the types of inference we are interested in are the following. Determine if there is evidence of a meaningful linear relationship in the data. If \\(\\beta_1 = 0\\), then there is no relation between \\(X\\) and \\(E\\{Y\\}\\). Hence we might be interested in testing the hypotheses \\[ H_0: \\beta_1 = 0 \\] \\[ H_a: \\beta_1 \\neq 0 \\] Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) is some hypothesized number. To provide a confidence interval for the true value of \\(\\beta_1\\). Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the sampling distribution of the estimate \\(b_1\\) of the parameter \\(\\beta_1\\). And, while we are at it, we may as well come to understand the sampling distribution of the estimate \\(b_0\\) of the parameter \\(\\beta_0\\). Review sampling distributions from Math 221. Since \\(b_1\\) is an estimate, it will vary from sample to sample, even though the truth, \\(\\beta_1\\), remains fixed. (The same holds for \\(b_0\\) and \\(\\beta_0\\).) It turns out that the sampling distribution of \\(b_1\\) (where the \\(X\\) values remain fixed from study to study) is normal with mean and variance: \\[ \\mu_{b_1} = \\beta_1 \\] \\[ \\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} \\] ## Simulation to Show relationship between Standard Errors ##----------------------------------------------- ## Edit anything in this area... n &lt;- 100 #sample size Xstart &lt;- 30 #lower-bound for x-axis Xstop &lt;- 100 #upper-bound for x-axis beta_0 &lt;- 2 #choice of true y-intercept beta_1 &lt;- 3.5 #choice of true slope sigma &lt;- 13.8 #choice of st. deviation of error terms ## End of Editable area. ##----------------------------------------------- # Create X, which will be used in the next R-chunk. X &lt;- rep(seq(Xstart,Xstop, length.out=n/2), each=2) ## After playing this chunk, play the next chunk as well. To see that this is true, consider the regression model with values specified for each parameter as follows. \\[ Y_i = \\overbrace{\\beta_0}^{2} + \\overbrace{\\beta_1}^{3.5} X_i + \\epsilon_i \\quad \\text{where} \\ \\epsilon_i \\sim N(0, \\overbrace{\\sigma^2}^{\\sigma=13.8}) \\] Using the equations above for \\(\\mu_{b_1}\\) and \\(\\sigma^2_{b_1}\\) we obtain that the mean of the sampling distribution of \\(b_1\\) will be \\(\\mu_{b_1} = \\beta_1 = 3.5\\) Further, we see that the variance of the sampling distribution of \\(b_1\\) will be \\(\\sigma^2_{b_1} = \\frac{\\sigma^2}{\\sum(X_i-\\bar{X})^2} = \\frac{13.8^2}{4.25\\times 10^{4}}\\) Taking the square root of the variance, the standard deviation of the sampling distribution of \\(b_1\\) will be \\(\\sigma_{b_1} = 0.067\\). That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of \\(b_1\\) and \\(b_0\\) for that regression. After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of \\(b_0\\) is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of \\(b_1\\) is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of \\(\\beta_0\\) or \\(\\beta_1\\), respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of \\(\\sigma_{b_0}\\) and \\(\\sigma_{b_1}\\), respectively. Amazing! 9.1 t Tests Using the information above about the sampling distributions of \\(b_1\\) and \\(b_0\\), an immediate choice of statistical test to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\] where \\(\\beta_{10}\\) can be zero, or any other value, is a t test given by \\[ t = \\frac{b_1 - \\beta_{10}}{s_{b_1}} \\] where \\(s^2_{b_1} = \\frac{MSE}{\\sum(X_i-\\bar{X})^2}\\). (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that \\(t\\) is distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. The nearly identical test statistic for testing \\[ H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_0 \\neq \\beta_{00} \\] is given by \\[ t = \\frac{b_0 - \\beta_{00}}{s_{b_0}} \\] where \\(s^2_{b_0} = MSE\\left[\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum(X_i-\\bar{X})^2}\\right]\\). This version of \\(t\\) has also been shown to be distributed as a \\(t\\) distribution with \\(n-2\\) degrees of freedom. 9.2 Confidence Intervals Creating a confidence interval for either \\(\\beta_1\\) or \\(\\beta_0\\) follows immediately from these results using the formulas \\[ b_1 \\pm t^*_{n-2}\\cdot s_{b_1} \\] \\[ b_0 \\pm t^*_{n-2}\\cdot s_{b_0} \\] where \\(t^*_{n-2}\\) is the critical value from a t distribution with \\(n-2\\) degrees of freedom corresponding to the chosen confidence level. 9.3 F tests Another way to test the hypotheses \\[ H_0: \\beta_1 = \\beta_{10} \\quad\\quad \\text{or} \\quad\\quad H_0: \\beta_0 = \\beta_{00} \\] \\[ H_a: \\beta_1 \\neq \\beta_{10} \\quad\\quad \\ \\ \\quad \\quad H_a: \\beta_0 \\neq \\beta_{00} \\] is with an \\(F\\) Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an \\(F\\) test is very general and can be used in many places that a t test cannot. In its most general form, the \\(F\\) test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that \\(H_0:\\beta_1=0\\) against the alternative that \\(H_a: \\beta_1\\neq 0\\), we are essentially comparing two models against each other. If \\(\\beta_1=0\\), then the corresponding model would be \\(E\\{Y_i\\} = \\beta_0\\). If \\(\\beta_1\\neq0\\), then the model remains \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the \\(F\\) Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example. Say we wanted to test the hypothesis that \\(H_0:\\beta_1 = 2.5\\) against the alternative that \\(\\beta_1\\neq2.5\\). Then the null, or reduced model, would be \\(E\\{Y_i\\}=\\beta_0+2.5X_i\\). The alternative, or full model, would be \\(E\\{Y_i\\}=\\beta_0+\\beta_1X_i\\). Thus, the null (reduced) model contains only one “free” parameter because \\(\\beta_1\\) has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model. Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing \\(H_0: \\beta_1=0\\) against \\(H_a:\\beta_1\\neq0\\) we have the partition \\[ \\underbrace{Y_i-\\bar{Y}}_{Total} = \\underbrace{\\hat{Y}_i - \\bar{Y}}_{Regression} + \\underbrace{Y_i-\\hat{Y}_i}_{Error} \\] The reason we use \\(\\bar{Y}\\) for the null model is that \\(\\bar{Y}\\) is the unbiased estimator of \\(\\beta_0\\) for the null model, \\(E\\{Y_i\\} = \\beta_0\\). Thus we would compute the following sums of squares: \\[ SSTO = \\sum(Y_i-\\bar{Y})^2 \\] \\[ SSR = \\sum(\\hat{Y}_i-\\bar{Y})^2 \\] \\[ SSE = \\sum(Y_i-\\hat{Y}_i)^2 \\] and note that \\(SSTO = SSR + SSE\\). Important to note is that \\(SSTO\\) uses the difference between the observations \\(Y_i\\) and the null (reduced) model. The \\(SSR\\) uses the diffences between the alternative (full) and null (reduced) model. The \\(SSE\\) uses the differences between the observations \\(Y_i\\) and the alternative (full) model. From these we could set up a General \\(F\\) table of the form Sum Sq Df Mean Sq F Value Model Error \\(SSR\\) \\(df_R-df_F\\) \\(\\frac{SSR}{df_R-df_F}\\) \\(\\frac{SSR}{df_R-df_F}\\cdot\\frac{df_F}{SSE}\\) Residual Error \\(SSE\\) \\(df_F\\) \\(\\frac{SSE}{df_F}\\) Total Error \\(SSTO\\) \\(df_R\\) "],
["lowess-and-loess-curves.html", "10 Lowess (and Loess) Curves", " 10 Lowess (and Loess) Curves A non-parametric approach to estimating \\(E\\{Y_i\\}\\)… Robust locally weighted regression and smoothing scatterplots (LOWESS), is an effective way to visually model the average y-value. Using Base R air2 &lt;- na.omit(select(airquality, Temp, Ozone)) # Just quickly draw the lowess curve: plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;) lines(lowess(air2$Ozone, air2$Temp), col=&quot;firebrick&quot;) ## OR optionally, ## allow for predictions as well as the graph: # plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;) # air2 &lt;- arrange(air2, desc(Ozone)) # mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1) # lines(mylo$fit ~ Ozone, data=air2) Using ggplot2 air2 &lt;- na.omit(select(airquality, Temp, Ozone)) # Just quickly draw the lowess curve: ggplot(air2, aes(x=Ozone, y=Temp)) + geom_point(color=&quot;darkgray&quot;) + geom_smooth(se=F, method=&quot;loess&quot;, method.args = list(degree=1)) + #Note, degree=2 by default. theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; ## OR optionally, ## allow for predictions as well as the graph: # air2 &lt;- arrange(air2, desc(Ozone)) # mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1) # ggplot(air2, aes(x=Ozone, y=Temp)) + # geom_point() + # geom_line(data=air2, aes(y=mylo$fit, x=Ozone)) Advantages Disadvantages Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data. No mathematical model. Not interpretable. No p-values. No adjusted R-squared. How it Works The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”. As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job. Study this graphic and the explanations below to learn how it works. Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics. X &lt;- cars$speed Y &lt;- cars$dist X &lt;- X[!is.na(X) &amp; !is.na(Y)] Y &lt;- Y[!is.na(X) &amp; !is.na(Y)] f &lt;- 1/2 n &lt;- length(X) lfit &lt;- rep(NA,n) for (xh in 1:n){ xdists &lt;- X - X[xh] nn &lt;- floor(n*f) r &lt;- sort(abs(xdists))[nn] xdists.nbrhd &lt;- which(abs(xdists) &lt; r) w &lt;- rep(0, length(xdists)) w[xdists.nbrhd] &lt;- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), col=rgb(.2,.2,.2,.3), cex=1.5, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;&quot;, ylab=&quot;&quot;) points(Y[xh] ~ X[xh], pch=16, col=&quot;orange&quot;) lmc &lt;- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE) lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;) #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2)) cat(&quot;\\n\\n&quot;) readline(prompt=paste0(&quot;Center point is point #&quot;, xh, &quot;... Press [enter] to continue...&quot;)) MADnotThereYet &lt;- TRUE count &lt;- 0 while(MADnotThereYet){ readline(prompt=paste0(&quot;\\n Adjusting line to account for outliers in the y-direction... Press [enter] to continue...&quot;)) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;wheat&quot;, add=TRUE) MAD &lt;- median(abs(lmc$res)) resm &lt;- lmc$res/(6*MAD) resm[resm&gt;1] &lt;- 1 bisq &lt;- (1-resm^2)^2 w &lt;- w*bisq obs &lt;- coef(lmc) lmc &lt;- lm(Y ~ X, weights=w) curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE) count &lt;- count + 1 if ( (sum(abs(obs-lmc$coef))&lt;.1) | (count &gt; 3)) MADnotThereYet &lt;- FALSE } curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;green&quot;, add=TRUE) points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=&quot;green&quot;) readline(prompt=paste0(&quot;\\n Use final line to get fitted value for this point... Press [enter] to continue to next point...&quot;)) lfit[xh] &lt;- predict(lmc, data.frame(X=X[xh])) lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;) if (xh == n){ readline(prompt=paste0(&quot;\\n Press [enter] to see actual Lowess curve...&quot;)) lines(lowess(X,Y, f=f), col=&quot;firebrick&quot;) legend(&quot;topleft&quot;, bty=&quot;n&quot;, legend=&quot;Actual lowess Curve using lowess(...)&quot;, col=&quot;firebrick&quot;, lty=1) } } Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The lowess function in R uses “f=2/3” and the loess function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want. Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue. Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight \\(w\\) given to point \\(j\\) of the neighborhood of points is defined by \\[ w_j = \\left(1- \\left( \\frac{|X_c - X_j|}{\\max_k |X_c - X_k|}\\right)^3\\right)^3 \\] where \\(X_c\\) is the x-value of the “center” dot and \\(X_j\\) is the x-value of any other dot in the neighborhood. The fitted-value of \\(\\hat{Y}_c\\) is obtained for the center point \\(X_c\\) of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression. Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps. Compute all residuals for points in the neighborhood of the current regression, denoted by \\(r_i\\). Then compute the MAD, median absolute deviation, of the residuals \\(MAD = \\text{median} (|r_1|, |r_2|, \\ldots)\\). Divide all residuals by 6 times the MAD: \\(u_i = r_i/(6\\cdot MAD)\\) (If \\(r_i &gt; 6\\cdot MAD\\) then set \\(u_i = 0\\).) Compute what are called bisquare weights using the formula: \\(b_i = (1 - u_i^2)^2\\) Perform a regression using the weights \\(w_i = w_i b_i\\) Repeat the above process with the new weights \\(w_i\\) until the weights stop changing very much. The final fitted values for each \\(X\\)-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve. Note that the default of the loess function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the loess option of “degree=2” (quadratic fits) or “degree = 1”. In the lowess function only a linear regression in each neighborhood is allowed. "]
]
